ğŸ“„ RAG Document Chatbot

A Retrieval-Augmented Generation (RAG) based document chatbot that allows users to upload documents and ask questions using semantic search and LLM-based answer generation.

ğŸ”§ Tech Stack

Backend: FastAPI

Frontend: Streamlit

Vector Store: FAISS

Embeddings: Sentence Transformers

LLM: Ollama (local inference)

Language: Python 3.10+

ğŸ“‚ Project Structure
rag-document-chatbot/
â”œâ”€â”€ app.py
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ ingest/
â”œâ”€â”€ rag/
â”œâ”€â”€ vector_store/
â”œâ”€â”€ data/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

âš™ï¸ Installation & Setup
1ï¸âƒ£ Clone Repository
git clone https://github.com/your-username/rag-document-chatbot.git
cd rag-document-chatbot

2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate   # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

â–¶ï¸ Run the Application
Start FastAPI Backend
uvicorn app:app --reload


Backend runs at:

http://127.0.0.1:8000

Start Streamlit Frontend
streamlit run streamlit_app.py

ğŸ§  How It Works

Document Loader loads PDF files

Chunker splits text into semantic chunks

Embedder converts chunks into embeddings

FAISS stores vectors for similarity search

Retriever fetches relevant context

Generator uses LLM to answer user queries

ğŸ“Œ Features

Offline RAG system

Local LLM via Ollama

Fast semantic search

Clean modular architecture

Easy deployment

ğŸ¥ Demo

See demo_video.md for demo link and walkthrough.

ğŸ“Œ Future Enhancements

Multi-document upload

Chat history

Hybrid search (BM25 + FAISS)

Cloud deployment (Docker)

ğŸ‘©â€ğŸ’» Author

Shibani M â€“ CSE
AI | NLP | RAG Systems | Backend Development

âœ… Deployment Notes (Important)

âœ” This WILL work in deployment if:

Ollama is installed on server OR

You replace it with OpenAI / HuggingFace API

FAISS index is rebuilt in production

âœ” For cloud:

Use Docker

Replace 127.0.0.1 with service hostname

Disable --reload