ğŸ“„ RAG-Based Document Question Answering Chatbot

ğŸ” Problem Statement
Large Language Models (LLMs) are powerful but cannot reliably answer questions about private or custom documents such as PDFs, reports, or internal notes because these documents are not part of their training data. Directly prompting an LLM with an entire document is inefficient, costly, and prone to hallucinations due to context window limitations.

This project solves the problem by building a Retrieval-Augmented Generation (RAG) based chatbot that allows users to ask natural language questions about a document and receive accurate, context-grounded answers.

ğŸ¯ Objective

Enable question answering over PDF documents

Prevent hallucinations by grounding answers in retrieved document context

Use modern embedding models and vector search

Demonstrate a clean, modular RAG architecture suitable for production
ğŸ§  Solution Overview (RAG Architecture)

This system follows a Retrieval-Augmented Generation (RAG) pipeline:
PDF Document
   â†“
Text Extraction
   â†“
Chunking
   â†“
Embedding Generation
   â†“
Vector Store (FAISS)
   â†“
Query Embedding
   â†“
Semantic Retrieval
   â†“
LLM Answer Generation
   â†“
User Interface

Key Idea

Instead of sending the entire document to the LLM, the system:

Retrieves only the most relevant chunks using vector similarity search

Injects those chunks into the LLM prompt

Forces the model to answer only from retrieved context

This improves accuracy, efficiency, and trustworthiness.

ğŸ—ï¸ Project Architecture
rag-chatbot/
â”‚
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ embedder.py        # Generates embeddings using SentenceTransformer
â”‚   â”œâ”€â”€ indexer.py         # Loads FAISS vector index
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ retriever.py       # Retrieves relevant chunks from FAISS
â”‚   â”œâ”€â”€ generator.py      # Generates answers using LLM (Ollama)
â”‚
â”œâ”€â”€ vector_store/
â”‚   â”œâ”€â”€ index.faiss        # FAISS vector index
â”‚   â”œâ”€â”€ chunks.pkl        # Stored text chunks
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ load_pdf.py        # PDF text extraction
â”‚
â”œâ”€â”€ api.py                 # FastAPI backend
â”œâ”€â”€ streamlit_app.py       # Streamlit frontend
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


ğŸ”§ Tech Stack

Backend: FastAPI

Frontend: Streamlit

Vector Store: FAISS

Embeddings: Sentence Transformers

LLM: Ollama (local inference)

Language: Python 3.10+



âš™ï¸ Installation & Setup
1ï¸âƒ£ Clone Repository
git clone https://github.com/your-username/rag-document-chatbot.git
cd rag-document-chatbot

2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate   # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

â–¶ï¸ Run the Application
Start FastAPI Backend
uvicorn app:app --reload


Backend runs at:

http://127.0.0.1:8000

Start Streamlit Frontend
streamlit run streamlit_app.py

ğŸš€ How It Works (Runtime Flow)

User enters a question in the Streamlit UI

Question is sent to FastAPI backend

Backend embeds the query

FAISS retrieves top-k relevant chunks

Retrieved context is injected into the LLM prompt

LLM generates a grounded answer

Answer + retrieved context is shown to the user

ğŸ“Œ Features

Offline RAG system

Local LLM via Ollama

Fast semantic search

Clean modular architecture

Easy deployment

ğŸ¥ Demo

See demo_video.md for demo link and walkthrough.

ğŸ“Œ Future Enhancements

Multi-document upload

Chat history

Hybrid search (BM25 + FAISS)

Cloud deployment (Docker)

ğŸ‘©â€ğŸ’» Author

Shibani M â€“ CSE III yr
AI | NLP | RAG Systems | Backend Development

âœ… Deployment Notes (Important)

âœ” This WILL work in deployment if:

Ollama is installed on server OR

You replace it with OpenAI / HuggingFace API

FAISS index is rebuilt in production

âœ” For cloud:

Use Docker

Replace 127.0.0.1 with service hostname

Disable --reload
